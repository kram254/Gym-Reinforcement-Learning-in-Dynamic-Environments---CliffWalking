{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "    COMP4240/5435 - Reinforcement Learning\n",
    "    \n",
    "# Homework 5 - Temporal Difference\n",
    "\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: ______________________ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to study different properties of Temporal Difference methods.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Notes:**\n",
    "- Questions marked with * are optional for COMP4240 - Undergraduate section. Questions marked as extra credit are optional for everyone. This homework does not include a separate extra credit question. However, students in COMP4240 can gain some extra credits by solving the last part.\n",
    "- Do not use a mix of python lists and numpy arrays. Every vector or matrix in your code should be a numpy array. \n",
    "- For functions that exist in both the python core and the numpy library, use the one in the numpy library. For example, use `np.max` instead of `max`. Another example: use `np.random.normal` instead of `random.gauss`.\n",
    "- Make sure all of your plots have a proper size and include `xlabel`, `ylabel`, `legend`, `title`, and `grid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are allowed to use the following modules\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Cliff Walking from gym\n",
    "\n",
    "**Description**\n",
    "\n",
    "The board is a 4x12 matrix, with (using NumPy matrix indexing):\n",
    "\n",
    "- `[3, 0]` as the start at bottom-left\n",
    "- `[3, 11]` as the goal at bottom-right\n",
    "- `[3, 1..10]` as the cliff at bottom-center\n",
    "\n",
    "If the agent steps on the cliff, it returns to the start. An episode terminates when the agent reaches the goal.\n",
    "Actions\n",
    "\n",
    "**Action**\n",
    "\n",
    "There are 4 discrete deterministic actions:\n",
    "\n",
    "- 0: move up\n",
    "- 1: move right\n",
    "- 2: move down\n",
    "- 3: move left\n",
    "\n",
    "**Observations**\n",
    "\n",
    "There are 3x12 + 1 possible states. In fact, the agent cannot be at the cliff, nor at the goal (as this results in the end of the episode). It remains all the positions of the first 3 rows plus the bottom-left cell. The observation is simply the current position encoded as flattened index.\n",
    "\n",
    "**Reward**\n",
    "\n",
    "Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward.\n",
    "\n",
    "**Setting the environment**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state: 36\n",
      "action: 2, next_state: 36, reward: -1\n",
      "action: 3, next_state: 36, reward: -1\n",
      "action: 0, next_state: 24, reward: -1\n",
      "action: 2, next_state: 36, reward: -1\n",
      "action: 1, next_state: 36, reward: -100\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CliffWalking-v0')\n",
    "\n",
    "#or if you get a warning use this instead \n",
    "\n",
    "# env = gym.make('CliffWalking-v0', new_step_api=True)\n",
    "\n",
    "observation, info = env.reset()\n",
    "print(f'initial state: {observation}')\n",
    "for _ in range(5):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f'action: {action}, next_state: {observation}, reward: {reward}')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I\n",
    "\n",
    "**(a)** Implement SARSA (on-policy TD control) using $\\varepsilon$–greedy policy with parameters $\\varepsilon=0.1$ and $Q_0 (s,a)=0$ for all $s,a$. Apply your implementation to the undiscounted cliff walking task for 50 independent runs where each run includes 500 episodes. For the last episode use `render_mode='human'` to watch the agent's learned behavior. You do not need to include this in the report. However, this is a good way of checking on the agent's learning progress a few times during the learning process.\n",
    "\n",
    "\n",
    "**(b)** Implement Q-learning (off-policy TD control) using $\\varepsilon$–greedy policy with parameters $\\varepsilon=0.1$ and $Q_0 (s,a)=0$ for all $s,a$. Apply your implementation to the undiscounted cliff walking task. Apply your implementation to the undiscounted cliff walking task for 50 independent runs where each run includes 500 episodes. For the last episode use `render_mode='human'` to watch the agent's learned behavior. You do not need to include this in the report. However, this is a good way of checking on the agent's learning progress a few times during the learning process.\n",
    "\n",
    "\n",
    "**(c)** Make sure both algorithms use the same parameters (e.g., $\\varepsilon$, $\\alpha$). Plot sum of rewards during episode over the number of episodes for both algorithms by averaging the results over the 50 runs. Both plots should be shown in one figure for comparison.\n",
    "\n",
    "\n",
    "**(d)** Print the optimal policy for each algorithm. This should be a matrix of size 4x12 with elements indicating optimal actions (either use 'U', 'D', 'R', 'L' or print corresponding arrows).\n",
    "\n",
    "\n",
    "**(e)** Re-run the whole experiment, this time by using a decaying $\\varepsilon$. Plot sum of rewards during espisode over the number of episodes. Print the optimal policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Your code here ---#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "\n",
    "a. What value of $\\alpha$ did you pick? Why?\n",
    "> Answer\n",
    "\n",
    "b. How similar the optimal policies get when you use a decaying $\\varepsilon$? Why?\n",
    "> Answer\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "pic2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II (*)\n",
    "Consider the following gridworld with four actions (up, down, right, and left). If the action takes the agent off the gird, the agent stays in the same state. In each non-terminating step, the agent receives a random reward of -12 or +10 with equal probability. The reward for reaching the goal state is +5 and the episode ends when the agent reaches the goal.\n",
    "\n",
    "Use $\\varepsilon$–greedy policy with $\\varepsilon(s) = \\frac{1}{\\sqrt{n(s)}}$ where $n(s)$ is the number of times state $s$ has been visited, assuring infinite exploration in the limit which is a theoretical requirement for the convergence of both **Q-learning** and **Double Q-learning**. \n",
    "![pic2.png](attachment:pic2.png)\n",
    "\n",
    "a. Implement **Q-learning** and **Double Q-learning** and apply them to this problem for 10,000 experiments using the learning rate $\\alpha=\\frac{1}{n(s,a)}$.\n",
    "\n",
    "b. Plot the average reward per step vs. number of time steps averaged over 10,000 experiments. The length of an episode following the optimal policy is five actions, so the optimal average reward per step is +0.2. Plot this true value in your figure and see how close your algorithm gets to the true value.\n",
    "\n",
    "c. Plot the maximal action value in the starting state $S$ (i.e. $max_a Q(s,a)$) averaged over 10,000 experiments. The optimal value of maximally valued action in the starting state is $5\\gamma^4 - \\sum_{k=0}^3 \\approx 0.36$. Plot this true value in your figure and see how close your algorithm gets to the true value.\n",
    "\n",
    "d. Repeat the experiments with $\\alpha = \\frac{1}{n(s,a)^{0.8}}$ and redo steps b and c.\n",
    "\n",
    "\n",
    "**Note:** You should have four figures (average rewards and maximal action values for different learning rates, $2 \\times 2$). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\admin\\Downloads\\New_folder\\Python\\jupiter_notebooks\\juoiter\\Homework_5 (2).ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Downloads/New_folder/Python/jupiter_notebooks/juoiter/Homework_5%20%282%29.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m epsilon \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m sqrt(N_STATE_VISITS[state])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Downloads/New_folder/Python/jupiter_notebooks/juoiter/Homework_5%20%282%29.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# Epsilon-greedy action\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/admin/Downloads/New_folder/Python/jupiter_notebooks/juoiter/Homework_5%20%282%29.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margmax(Q[state]) \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand() \u001b[39m>\u001b[39m epsilon \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m4\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Downloads/New_folder/Python/jupiter_notebooks/juoiter/Homework_5%20%282%29.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m N_STATE_ACTION_VISITS[state][action] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Downloads/New_folder/Python/jupiter_notebooks/juoiter/Homework_5%20%282%29.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# Update learning rate\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\Downloads\\New_folder\\Python\\jupiter_notebooks\\juoiter\\Venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m-> 1229\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39m\u001b[39margmax\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\admin\\Downloads\\New_folder\\Python\\jupiter_notebooks\\juoiter\\Venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#--- Your code here ---#\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "\n",
    "# Environment\n",
    "ROWS, COLS = 4, 4\n",
    "START, GOAL = (3, 0), (0, 0)\n",
    "ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "\n",
    "# Variables to store data for both learning rates\n",
    "average_rewards_1, average_rewards_08 = [], []\n",
    "max_action_values_1, max_action_values_08 = [], []\n",
    "\n",
    "# Run experiments for both learning rates\n",
    "for learning_rate_type in ['1', '0.8']:\n",
    "    # Initialize\n",
    "    N_STATE_VISITS = np.zeros((ROWS, COLS))\n",
    "    N_STATE_ACTION_VISITS = np.zeros((ROWS, COLS, 4))\n",
    "    Q = np.zeros((ROWS, COLS, 4))\n",
    "    \n",
    "    average_rewards = []\n",
    "    max_action_values = []\n",
    "\n",
    "    # Main Loop\n",
    "    for episode in range(10000):\n",
    "        state = START\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        while state != GOAL:\n",
    "            N_STATE_VISITS[state] += 1\n",
    "            epsilon = 1 / sqrt(N_STATE_VISITS[state])\n",
    "\n",
    "            # Epsilon-greedy action\n",
    "            action = np.argmax(Q[state]) if np.random.rand() > epsilon else np.random.randint(4)\n",
    "            N_STATE_ACTION_VISITS[state][action] += 1\n",
    "            \n",
    "            # Update learning rate\n",
    "            alpha = 1 / (N_STATE_ACTION_VISITS[state][action] ** float(learning_rate_type))\n",
    "\n",
    "            # Update state and reward\n",
    "            new_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])\n",
    "            new_state = (max(min(new_state[0], 3), 0), max(min(new_state[1], 3), 0))\n",
    "            reward = np.random.choice([-12, 10]) if new_state != GOAL else 5\n",
    "\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            # Q-Learning update\n",
    "            best_q = np.max(Q[new_state])\n",
    "            Q[state][action] += alpha * (reward + best_q - Q[state][action])\n",
    "            state = new_state\n",
    "\n",
    "        average_rewards.append(total_reward / steps)\n",
    "        max_action_values.append(np.max(Q[START]))\n",
    "\n",
    "    if learning_rate_type == '1':\n",
    "        average_rewards_1, max_action_values_1 = average_rewards, max_action_values\n",
    "    else:\n",
    "        average_rewards_08, max_action_values_08 = average_rewards, max_action_values\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "# Average reward per step with alpha = 1/n(s,a)\n",
    "ax[0, 0].plot(average_rewards_1)\n",
    "ax[0, 0].axhline(0.2, color='r', linestyle='--', label=\"Optimal Value\")\n",
    "ax[0, 0].set_title('Average Reward per Step (alpha = 1/n(s,a))')\n",
    "ax[0, 0].set_xlabel('Episodes')\n",
    "ax[0, 0].set_ylabel('Average Reward')\n",
    "ax[0, 0].legend()\n",
    "\n",
    "# Maximal action value at start state with alpha = 1/n(s,a)\n",
    "ax[0, 1].plot(max_action_values_1)\n",
    "optimal_value = 5 - sum([1 for _ in range(4)])\n",
    "ax[0, 1].axhline(optimal_value, color='r', linestyle='--', label=\"Optimal Value\")\n",
    "ax[0, 1].set_title('Maximal Action Value at Start (alpha = 1/n(s,a))')\n",
    "ax[0, 1].set_xlabel('Episodes')\n",
    "ax[0, 1].set_ylabel('Maximal Action Value')\n",
    "ax[0, 1].legend()\n",
    "\n",
    "# Average reward per step with alpha = 1/n(s,a)^0.8\n",
    "ax[1, 0].plot(average_rewards_08)\n",
    "ax[1, 0].axhline(0.2, color='r', linestyle='--', label=\"Optimal Value\")\n",
    "ax[1, 0].set_title('Average Reward per Step (alpha = 1/n(s,a)^0.8)')\n",
    "ax[1, 0].set_xlabel('Episodes')\n",
    "ax[1, 0].set_ylabel('Average Reward')\n",
    "ax[1, 0].legend()\n",
    "\n",
    "# Maximal action value at start state with alpha = 1/n(s,a)^0.8\n",
    "ax[1, 1].plot(max_action_values_08)\n",
    "ax[1, 1].axhline(optimal_value, color='r', linestyle='--', label=\"Optimal Value\")\n",
    "ax[1, 1].set_title('Maximal Action Value at Start (alpha = 1/n(s,a)^0.8)')\n",
    "ax[1, 1].set_xlabel('Episodes')\n",
    "ax[1, 1].set_ylabel('Maximal Action Value')\n",
    "ax[1, 1].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "\n",
    "a.\tWhich algorithm finds a better policy? Why?\n",
    "> Answer \n",
    "\n",
    "b.\tb.\tWhich learning rate performs better? \n",
    "> Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
